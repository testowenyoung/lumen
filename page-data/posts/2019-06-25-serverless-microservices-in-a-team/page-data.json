{"componentChunkName":"component---src-templates-blog-post-js","path":"/posts/2019-06-25-serverless-microservices-in-a-team/","result":{"data":{"site":{"siteMetadata":{"title":"Gatsby Starter Blog"}},"markdownRemark":{"id":"8601fcdc-4c56-5b75-b0b7-0d3a469e2b75","excerpt":"In a previous blog post, we looked at how we would structure our new services to allow for developers to write and debug their functions locally as easily as…","html":"<p>In a <a href=\"https://serverless.com/blog/serverless-local-development\">previous blog post</a>, we looked at how we would structure our new services to allow for developers to write and debug their functions locally as easily as possible. This included the setup of some unit testing frameworks and mocking tools so we didn’t have to call out to AWS services over the Internet. </p>\n<p>But if you are a developer in a team, you probably need to find ways to work with your team when developing your Serverless microservices. </p>\n<h4>Goals</h4>\n<p>To get started, just like the previous blog post, lets set some goals to meet for this blog post:</p>\n<ul>\n<li>Using VCS for our services</li>\n<li>Easy CI/CD setup</li>\n<li>Integration testing</li>\n<li>Going live</li>\n</ul>\n<h4>Using Version Control</h4>\n<p>Obviously, as with any software project, we need to use some form of version control and predominantly that’s git. And while there are divided opinions on the matter, I prefer the idea of keeping each service in its own git repo. A lot of proponents prefer the monrepo approach, but let me provide reasons why I prefer to keep each service entirely seperate:</p>\n<ul>\n<li>Promotes re-usability of a service if you can include it into any serverless application (that being a collection of microservices).</li>\n<li>Since we should be building our services to be as decoupled as possible, we should not need to include other services just to have ours working as intended.</li>\n<li>Forces atomicity of our services, meaning that each service is built to contain everything it needs to function. All DynamoDB tables, all S3 buckets. And they are not shared across services.</li>\n<li>Since we are forced to keep services decoupled and atomic, synchronous communications are discouraged (since they add hard dependancies) and asynchronous communications are encouraged to pass data between services via PubSub or message queues.</li>\n<li>Each service can be super specialised in its architecture based on the need it fulfils. Even a different language runtime.</li>\n<li>Local development is lighter since you will only need to pull from the repo and run locally whichever microservice you are currently working on and not have to have a local version of all 100 or more services your team has developed.</li>\n<li>If each service is a git repo on its own that means that managing the collection of services is relatively easy when it comes to setting up deployment. No complicated setup to deploy each individual service that has been combined under a single repo. Each service is a mini application all on its own.</li>\n</ul>\n<p>If we assume that your services are built as per the previous blog post about setting up a local development environment, then we also have a very easy method to have other developers get involved with maintaining any of our services. The process looks like so for a developer looking to work on an existing service:</p>\n<ul>\n<li>Fork the repo for the service to maintain.</li>\n<li>Clone the forked repo to your local</li>\n<li>Run <code class=\"language-text\">npm install</code> to setup all local dependancies.</li>\n<li>Create branch off of develop to do work in.</li>\n<li>Develop locally as normal using the unit testing and mocking environment to execute and debug.</li>\n<li>Deploy to personal AWS account for some final integration testing with AWS services.</li>\n<li>Push branch with new feature/bug fix to forked repo.</li>\n<li>Create pull request to the original repo.</li>\n</ul>\n<p>Now its up to the team around the service to decide whether to accept the pull request after a code review. And the best part is that you can also do your own CI/CD setup at that point to run unit and integration tests as you wish. Since a developer will be creating a branch off of develop, their PR is back onto develop.</p>\n<h4>Continuous Integration and Deployment</h4>\n<p>I really love simple continuous integration setups. And I love portable setups. You don’t get much more simple and portable than a single yml file in the service root. I’m a bit of a fan of Gitlab, and one of the biggest reasons is because of their integration of CI/CD by default even into private and free projects. Of course, you can use whatever flavour of VCS and CI tools you wish, but this example leverages off of Gitlab and their runners system. Similar types of configurations exist for a lot of the CI/CD tools that exist.</p>\n<p>Gitlab allows you to create a <code class=\"language-text\">.gitlab-ci.yml</code> file that describes the CI/CD process for even multiple branches of your repository. Here is an example:</p>\n<div class=\"gatsby-highlight\" data-language=\"yml\"><pre class=\"language-yml\"><code class=\"language-yml\"><span class=\"token key atrule\">image</span><span class=\"token punctuation\">:</span> node<span class=\"token punctuation\">:</span>10<span class=\"token punctuation\">-</span>alpine <span class=\"token comment\">#Choose which docker image to base your container on</span>\n\n<span class=\"token key atrule\">before_script</span><span class=\"token punctuation\">:</span>\n  <span class=\"token punctuation\">-</span> npm install <span class=\"token comment\">#Make sure all local dependancies are installed</span>\n\n<span class=\"token key atrule\">staging_deploy</span><span class=\"token punctuation\">:</span>\n  <span class=\"token key atrule\">cache</span><span class=\"token punctuation\">:</span>\n    <span class=\"token key atrule\">paths</span><span class=\"token punctuation\">:</span>\n      <span class=\"token punctuation\">-</span> node_modules/ <span class=\"token comment\">#Cache our node_modules folder so its quicker to run</span>\n  <span class=\"token key atrule\">stage</span><span class=\"token punctuation\">:</span> deploy\n  <span class=\"token key atrule\">environment</span><span class=\"token punctuation\">:</span> staging\n  <span class=\"token key atrule\">script</span><span class=\"token punctuation\">:</span>\n    <span class=\"token punctuation\">-</span> npm install <span class=\"token punctuation\">-</span>g mocha\n    <span class=\"token punctuation\">-</span> mocha src/test/functions\n    <span class=\"token punctuation\">-</span> node_modules/serverless/bin/serverless config credentials <span class=\"token punctuation\">-</span><span class=\"token punctuation\">-</span>key AWS_ACCESS_KEY <span class=\"token punctuation\">-</span><span class=\"token punctuation\">-</span>secret AWS_ACCESS_SECRET <span class=\"token punctuation\">-</span><span class=\"token punctuation\">-</span>stage staging\n    <span class=\"token punctuation\">-</span> node_modules/serverless/bin/serverless deploy <span class=\"token punctuation\">-</span><span class=\"token punctuation\">-</span>stage staging\n    <span class=\"token comment\">#Run any other command that executes your integration tests across your entire application if you wish</span>\n  <span class=\"token key atrule\">only</span><span class=\"token punctuation\">:</span>\n    <span class=\"token punctuation\">-</span>  develop\n\n<span class=\"token key atrule\">production_deploy</span><span class=\"token punctuation\">:</span>\n  <span class=\"token key atrule\">cache</span><span class=\"token punctuation\">:</span>\n    <span class=\"token key atrule\">paths</span><span class=\"token punctuation\">:</span>\n      <span class=\"token punctuation\">-</span> node_modules/\n  <span class=\"token key atrule\">stage</span><span class=\"token punctuation\">:</span> deploy\n  <span class=\"token key atrule\">environment</span><span class=\"token punctuation\">:</span> prod\n  <span class=\"token key atrule\">script</span><span class=\"token punctuation\">:</span>\n    <span class=\"token punctuation\">-</span> node_modules/serverless/bin/serverless config credentials <span class=\"token punctuation\">-</span><span class=\"token punctuation\">-</span>key AWS_ACCESS_KEY <span class=\"token punctuation\">-</span><span class=\"token punctuation\">-</span>secret AWS_ACCESS_SECRET <span class=\"token punctuation\">-</span><span class=\"token punctuation\">-</span>stage prod\n    <span class=\"token punctuation\">-</span> node_modules/serverless/bin/serverless deploy <span class=\"token punctuation\">-</span><span class=\"token punctuation\">-</span>stage prod\n  <span class=\"token key atrule\">only</span><span class=\"token punctuation\">:</span>\n    <span class=\"token punctuation\">-</span>  master</code></pre></div>\n<blockquote>\n<p>NOTE: If you use Serverless Framework Enterprise, there’s no need for the <code class=\"language-text\">serverless config credentials</code> lines as credentials for deployment are provided via Serverless Framework Enterprise when you call <code class=\"language-text\">serverless deploy</code>. If you are interested in how to get started with Serverless Framework Enterprise then <a href=\"https://github.com/serverless/enterprise/blob/master/docs/getting-started.md\">check out the docs here</a></p>\n</blockquote>\n<p>Here’s the quick rundown of the configuration above. It starts by you selecting which image from Dockerhub you wish your container to run on. You could also build your own image from scratch and store it on Gitlab itself and use that too so you have ultimate flexibility. Then <code class=\"language-text\">before_script</code> is executed before the rest of the file which just does an <code class=\"language-text\">npm install</code> to ensure all our dependancies are loaded.</p>\n<p>After that we have two sections; our <code class=\"language-text\">staging_deploy</code> and <code class=\"language-text\">production_deploy</code>. Each one <code class=\"language-text\">only</code> executes on specific branches; develop for <code class=\"language-text\">staging_deploy</code> and master for <code class=\"language-text\">production_deploy</code>.</p>\n<p>Its at this point that configurations can diverge. The staging configuration installs and run’s mocha to ensure that all unit tests pass before getting to deployment. We then have to setup our access details into the AWS account we want our staging environment to deploy to. This is accomplished by setting <code class=\"language-text\">AWS_ACCESS_KEY</code> and <code class=\"language-text\">AWS_ACCESS_SECRET</code> values in Gitlab manually with the access details from AWS as environment variables made available to the script in our container on every CI run. Finally, we do a deploy to the staging stage. As noted by the comment, after this you can then kick off any integration test you wish or even look into Gitlab’s sophisticated CI environment for more options; there’s a lot more to it than just this example.</p>\n<p>Regardless of which VCS and CI tool you use, you should be able to achieve similar results; Bitbucket has Pipelines that works similarly, Travis is popular with Github, Circle CI is a popular bet as well. Choose whatever floats your boat, the principles stay the same.</p>\n<h4>Integration testing</h4>\n<p>Integration testing is a funny beast, and can be tricky to master in ANY environment, not just a Serverless application environment. In the book “Building Microservices” by Sam Newman, the go to resource about building microservices, he describes how difficult it can be to accomplish a good integration testing strategy. Problems exist along the lines of keeping the correct versions of services in sync when running the test, resetting the test environment and for large applications, just the sheer amount of time it can take to get an environment setup and configured for each test before you even start running it, then waiting around for the results, spending all that time writing tests and tests becoming stale… Running a good integration testing strategy can be difficult and very time consuming. However, it doesn’t mean you shouldn’t try and wouldn’t be useful in your situation.</p>\n<p>Integration testing includes a LOT of possibilities. From simple API endpoint testing by just calling HTTP endpoints in sequence and assessing the results to full blown browser simulation by “clicking” on links and filling in forms. One thing all of these have in common is the need to do the testing in the cloud. And thankfully this gets easier to do when using stages with the Serverless Framework. Here is a sequence of steps I have seen used when attempting to provide integration tests to a code base on being taken live:</p>\n<ul>\n<li>A pull request submitted by a developer on the team gets merged into a services develop branch.</li>\n<li>A merge (potentially after peer review) triggers the start of the CI process (whether thats Gitlab CI/CD, Bitbucket Pipelines, CircleCI, Travis or any other CI/CD tool).</li>\n<li>Unit tests are executed on the service if they exist and if any fail, deployment fails.</li>\n<li>Altered service has a deployment occur by default to the <code class=\"language-text\">test</code> stage using <code class=\"language-text\">serverless deploy --stage test</code>.</li>\n<li>CI system waits for human intervention to decide if an integration test is required.</li>\n<li>If yes, an endpoint is called to a service dedicated for the purpose of resetting data on integration test stage to bootstrap for a new integration test run.</li>\n<li>Integration test (API endpoint or browser emulation) is executed.</li>\n<li>If successful (or if an integration was bypassed) develop gets merged into master and deployment occurs.</li>\n</ul>\n<p>This is naturally very simplified, and is only one of many possible configurations. As for the actual integration testing tool? There are many options here, everything from rolling your own with <code class=\"language-text\">mocha</code> or <code class=\"language-text\">jest</code> and the <code class=\"language-text\">supertest</code> module, to tools such as <a href=\"https://www.cypress.io/\">Cypress</a> and <a href=\"https://github.com/GoogleChrome/puppeteer\">Puppeteer</a> for that emulated browser testing.</p>\n<h4>Production</h4>\n<p>Going into production can be a little scary. However, even here we can help ameliorate the concern somewhat. Canary deployments is a feature added by AWS not so long ago and shortly thereafter, <a href=\"https://github.com/davidgf\">David García</a> authored a great Serverless plugin, the <a href=\"https://github.com/davidgf/serverless-plugin-canary-deployments\">serverless-plugin-canary-deployments</a> plugin. </p>\n<p>For anyone who is not aware what canary deployment means, its a means by which you can have only some of your traffic shift over to a new version of your Lambda function after deployment. The function is monitored for errors, and if none are found, then more traffic is moved over and monitoring continues. This continues until all traffic is switched over to the new version.</p>\n<p>However, if an error is detected, then the deployment is immediately rolled back to the known working previous version. </p>\n<p>There are a ton of options with this plugin, everything from setting up additional Lambda functions to execute prior to or after traffic shifting, custom alarms to trigger rollbacks besides just errors, etc. I highly recommend taking a good look at that plugin if you want to ensure that deployments into production go smoothly.</p>\n<h4>Conclusion</h4>\n<p>Serverless can and does work well in a team setting with features and options available to work together as a team (including <a href=\"https://serverless.com/enterprise/\">Serverless Framework Enterprise</a>). If you have any extra advice or tips to offer, please do so in the comments below or <a href=\"https://forum.serverless.com/\">drop by our forums</a> if you have any questions so the rest of the community can get involved.</p>","frontmatter":{"title":"Serverless Microservices in a Team","date":"June 25, 2019","description":"How do we build Serverless microservices as a team?"}}},"pageContext":{"slug":"/posts/2019-06-25-serverless-microservices-in-a-team/","previous":{"fields":{"slug":"/posts/2019-06-20-choosing-a-database-with-serverless/"},"frontmatter":{"title":"Choosing a Database for Serverless Applications"}},"next":{"fields":{"slug":"/posts/2019-06-27-dynamic-image-resizing-python/"},"frontmatter":{"title":"Dynamic image resizing with Python and Serverless framework"}}}}}