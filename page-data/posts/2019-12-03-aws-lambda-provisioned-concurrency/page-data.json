{"componentChunkName":"component---src-templates-blog-post-js","path":"/posts/2019-12-03-aws-lambda-provisioned-concurrency/","result":{"data":{"site":{"siteMetadata":{"title":"Gatsby Starter Blog"}},"markdownRemark":{"id":"2a2276c8-28aa-5733-a579-8b9a413703c9","excerpt":"AWS Lambda pretty much single-handedly kick-started the serverless movement we find ourselves in. A compute service with automated scaling and complete…","html":"<p><a href=\"https://www.serverless.com/aws-lambda/\">AWS Lambda</a> pretty much single-handedly kick-started the serverless movement we find ourselves in. A compute service with automated scaling and complete elimination of machine or container maintenance. However, some of the characteristics of the service made it a little less than desirable for certain workloads.</p>\n<p>If you were trying to use Lambda in a use case that was very latency sensitive, cold starts were probably your greatest concern. Cold starts have also been the biggest issue pointed out by detractors of service as to why you need to be cautious about adopting Lambda as your primary compute platform. However, thankfully, AWS has heard the concerns and has provided the means for us to solve the problem.</p>\n<p>If you have just deployed a Serverless service, or none of your functions have been invoked in some time, your functions will be cold. This means that if a new event trigger did occur to invoke a Lambda function, a brand new micro VM would need to be instantiated, the runtime loaded in, your code and all of its dependencies imported and finally your code executed; a process that could take 50 - 200 ms (or longer depending on the runtime you choose) before any execution actually started. However, after execution, this micro VM that took some time to spin up is kept available for afterwards for anywhere up to an hour and if a new event trigger comes in, then execution could begin immediately.</p>\n<p>Before now, if you were trying to use techniques to create warm Lambda instances, this was a tricky exercise. It was a little difficult to exactly control how many warm instances you wanted simultaneously and you then had to execute the Lambda you wanted with some kind of branching logic that determined whether this was a warm up execution or an actual execution. It was rather ugly. But it helped folks step past the cold start issues to some degree.</p>\n<p>However, AWS has now launched Provisioned Concurrency as a feature. It does pretty much the same thing as those Serverless Framework plugins that try to keep a certain number of warm functions running by allowing you configure warm instances right from the get go. In addition, there are no code changes needed. All we need to do is set a value as to how many provisioned instances we want for a specific function, and the <a href=\"https://www.serverless.com/aws-lambda/\">AWS Lambda</a> service itself will ensure to always have that quantity of warmed instances waiting for work!</p>\n<p>Combine this with the auto scaling features of Lambda and we now have the means to respond rapidly to traffic as well as automatically accommodate more traffic as it comes in.</p>\n<h4>AWS Console</h4>\n<p>This setting can be made very simply in the AWS Console. Go to the function in the Lambda service, scroll all the way to the bottom and set it at what you want the minimum provisioned concurrency to always be. Easy as that.</p>\n<h4>Serverless Framework</h4>\n<p>Of course, we don’t really want to dip into the console if our service is built with the Serverless Framework, so instead, we can change one setting on our functions definition to add provisioned concurrency to that function.</p>\n<div class=\"gatsby-highlight\" data-language=\"yml\"><pre class=\"language-yml\"><code class=\"language-yml\"><span class=\"token key atrule\">functions</span><span class=\"token punctuation\">:</span>\n  <span class=\"token key atrule\">hello</span><span class=\"token punctuation\">:</span>\n    <span class=\"token key atrule\">handler</span><span class=\"token punctuation\">:</span> handler.hello\n      <span class=\"token key atrule\">events</span><span class=\"token punctuation\">:</span>\n        <span class=\"token punctuation\">-</span> <span class=\"token key atrule\">http</span><span class=\"token punctuation\">:</span>\n            <span class=\"token key atrule\">path</span><span class=\"token punctuation\">:</span> /hello\n          <span class=\"token key atrule\">method</span><span class=\"token punctuation\">:</span> get\n    <span class=\"token key atrule\">provisionedConcurrency</span><span class=\"token punctuation\">:</span> <span class=\"token number\">5</span></code></pre></div>\n<p>In the example above, the <code class=\"language-text\">hello</code> Lambda function will always have 5 warm instances ready to go to handle incoming HTTP requests from API Gateway.</p>\n<h4>AWS API</h4>\n<p>However, it doesn’t end there. You can even go so far as to write a simple Lambda that you run on an hourly basis with a pre-determined schedule in mind. If you are like a lot of organsiations, you will have busy spikes you know about well in advance. In these situations, you may not want <code class=\"language-text\">provisionedConcurrency</code> all the time, but you may want it during those known spikes. </p>\n<p>Provisioned Concurrency can be set via the AWS SDK:</p>\n<div class=\"gatsby-highlight\" data-language=\"javascript\"><pre class=\"language-javascript\"><code class=\"language-javascript\"><span class=\"token string\">'use strict'</span><span class=\"token punctuation\">;</span>\n<span class=\"token keyword\">const</span> <span class=\"token constant\">AWS</span> <span class=\"token operator\">=</span> <span class=\"token function\">require</span><span class=\"token punctuation\">(</span><span class=\"token string\">'aws-sdk'</span><span class=\"token punctuation\">)</span>\n\nmodule<span class=\"token punctuation\">.</span>exports<span class=\"token punctuation\">.</span><span class=\"token function-variable function\">setProvisionedConcurrency</span> <span class=\"token operator\">=</span> <span class=\"token keyword\">async</span> <span class=\"token parameter\">event</span> <span class=\"token operator\">=></span> <span class=\"token punctuation\">{</span>\n  <span class=\"token keyword\">const</span> params <span class=\"token operator\">=</span> <span class=\"token punctuation\">{</span>\n    FunctionName<span class=\"token operator\">:</span> <span class=\"token string\">'MyFunctionName'</span><span class=\"token punctuation\">,</span>\n    ProvisionedConcurrentExecutions<span class=\"token operator\">:</span> <span class=\"token string\">'5'</span><span class=\"token punctuation\">,</span>\n    Qualifier<span class=\"token operator\">:</span> <span class=\"token string\">'aliasname'</span>\n  <span class=\"token punctuation\">}</span><span class=\"token punctuation\">;</span>\n  <span class=\"token keyword\">const</span> lambda <span class=\"token operator\">=</span> <span class=\"token keyword\">new</span> <span class=\"token class-name\">AWS</span>\n  lambda<span class=\"token punctuation\">.</span><span class=\"token function\">putProvisionedConcurrencyConfig</span><span class=\"token punctuation\">(</span>params<span class=\"token punctuation\">,</span> <span class=\"token keyword\">function</span><span class=\"token punctuation\">(</span><span class=\"token parameter\">err<span class=\"token punctuation\">,</span> data</span><span class=\"token punctuation\">)</span> <span class=\"token punctuation\">{</span>\n    <span class=\"token keyword\">if</span> <span class=\"token punctuation\">(</span>err<span class=\"token punctuation\">)</span> console<span class=\"token punctuation\">.</span><span class=\"token function\">log</span><span class=\"token punctuation\">(</span>err<span class=\"token punctuation\">,</span> err<span class=\"token punctuation\">.</span>stack<span class=\"token punctuation\">)</span><span class=\"token punctuation\">;</span> <span class=\"token comment\">// an error occurred</span>\n    <span class=\"token keyword\">else</span>     console<span class=\"token punctuation\">.</span><span class=\"token function\">log</span><span class=\"token punctuation\">(</span>data<span class=\"token punctuation\">)</span><span class=\"token punctuation\">;</span>           <span class=\"token comment\">// successful response</span>\n  <span class=\"token punctuation\">}</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">;</span>\n<span class=\"token punctuation\">}</span><span class=\"token punctuation\">;</span></code></pre></div>\n<p>Now you have the means to schedule the provisioned concurrency whenever you choose and so optimise the cost efficiency of it. </p>\n<h4>Conclusion</h4>\n<p>This single feature released by AWS gives those needing greater control over ensuring lower latencies exactly the tool they needed while keeping Lambda Serverless. You are not provisioning hardware or networks, runtimes and operating systems, but tweaking the settings that directly affect the end result in a measurable and predictable way. And this single feature opens Serverless up even further to more use cases and makes it far more competitive in the world of application development.</p>","frontmatter":{"title":"Provisioned Concurrency: What it is and how to use it with the Serverless Framework","date":"December 03, 2019","description":"If you were trying to use Lambda in a use case that was very latency sensitive, cold starts were probably your greatest concern. AWS has heard the concerns"}}},"pageContext":{"slug":"/posts/2019-12-03-aws-lambda-provisioned-concurrency/","previous":{"fields":{"slug":"/posts/2019-12-02-reinvent-2019-serverless-announcements/"},"frontmatter":{"title":"All the Serverless announcements at re:Invent 2019"}},"next":{"fields":{"slug":"/posts/2019-12-03-amazon-rds-proxy/"},"frontmatter":{"title":"Amazon RDS Proxy makes it easier to use SQL in Serverless"}}}}}