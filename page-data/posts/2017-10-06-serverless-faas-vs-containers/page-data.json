{"componentChunkName":"component---src-templates-blog-post-js","path":"/posts/2017-10-06-serverless-faas-vs-containers/","result":{"data":{"site":{"siteMetadata":{"title":"Gatsby Starter Blog"}},"markdownRemark":{"id":"f0040f6f-00a7-5104-bae2-68fef987f866","excerpt":"Contrary to popular thought, Serverless (FaaS) and Containers (Container Orchestration) have some pretty important things in common. You want a modern, future‚Ä¶","html":"<p>Contrary to popular thought, Serverless (FaaS) and Containers (Container Orchestration) have some pretty important things in common.</p>\n<p>You want a modern, future-proof architecture? They both have it. You want to build that slick architecture while also leveraging the latest innovations in distributed systems and large-scale application development? Yep, they both have that too.</p>\n<p>It makes it hard to decide which one is best for <em>you</em>. But friend, you deserve to know. So we‚Äôre taking off the gloves and laying it all on the line.</p>\n<p>What are the commonalities and distinctions? What are the advantages and disadvantages of each?</p>\n<p>It‚Äôs serverless computing vs containerization, right now. Read on.</p>\n<h4>How did we get here?</h4>\n<p>Before we jump right into the details, let‚Äôs cover some very important history.</p>\n<h5>1. Physical servers</h5>\n<p>We used to build our own infrastructure in the form of physical servers. We set up those machines, deployed our code on them, scaled them and maintained them. The whole thing was a manual process, and pretty slow to boot.</p>\n<h5>2. Server clusters and VMs</h5>\n<p>Using a single physical server for one application was a waste of resources. So, we evolved our infrastructure thinking and combined multiple physical servers into a cluster.</p>\n<p>We used those so-called ‚Äòvirtual machines‚Äô to run multiple applications in isolation on top of this infrastructure. Deployment and management got way faster and easier. However, server administration was still necessary and largely very manual.</p>\n<h5>3. Entering the cloud (IaaS)</h5>\n<p>Setting up and operating your own datacenter came with new operational challenges; cloud computing began to tackle those issues.</p>\n<p>Why not rent your servers and operational services individually, for a monthly fee? This approach made it way easier to scale up or down, and let teams move faster.</p>\n<h5>4. PaaS</h5>\n<p>While cloud environments made it convenient to build large-scale applications, they still came saddled with the downsides of manual administration:</p>\n<p><em>‚ÄúAre the latest security fixes installed?‚Äù</em></p>\n<p><em>‚ÄúWhen should we scale down/up?‚Äù</em></p>\n<p><em>‚ÄúHow many more servers do we need?‚Äù</em></p>\n<p>Wouldn‚Äôt it be great if all those administrative hassles were taken off of our plates, and we could simply focus on applications and business value?</p>\n<p>Yep! That‚Äôs what some other folks started thinking, too.</p>\n<h4>In corner 1: Containers</h4>\n<p>Wouldn‚Äôt it be nice if one could pack the application, with alllllll its dependencies, into a dedicated box and run it anywhere? No matter what software dependencies the host system has installed, or where and what the host system actually is?</p>\n<p>That‚Äôs the idea of containerization. Create a container which has all the required dependencies pre-installed, put your application code inside of it and run it everywhere the container runtime is installed. No more devs saying: ‚ÄúWell, it works on my machine!‚Äù</p>\n<p>Containerization gained attention when it came to light that Google used such technologies to power some of their services (such as Gmail or Maps). Using containers was initially pretty cumbersome, however; it required deep knowledge about Linux kernel internals and making home-grown scripts to put an application in a container and run it on a host machine.</p>\n<p>Then Dotcloud (a PaaS startup from San Francisco) announced a new tool called Docker at <a href=\"https://www.youtube.com/watch?v=wW9CAH9nSLs\">Pycon US 2013</a>. Docker was an easy to use CLI tool which made it possible to manage software containers easily.</p>\n<p>Dotcloud then pivoted to become Docker, and Google worked on an OpenSource implementation of the ‚ÄúBorg‚Äù container orchestration service, which is called Kubernetes.</p>\n<p>More and more enterprises adopted containers, and standards around this new technology got defined. Nowadays, nearly every cloud provider offers a way to host containerized applications on their infrastructure.</p>\n<h5>Advantages of containers</h5>\n<ul>\n<li>Control and Flexibility</li>\n<li>Vendor-agnostic</li>\n<li>Easier migration path</li>\n<li>Portability</li>\n</ul>\n<h5>Disadvantages of containers</h5>\n<ul>\n<li>Administrative work (e.g. apply security fixes for containers)</li>\n<li>Scaling is slower</li>\n<li>Running costs</li>\n<li>Hard to get started</li>\n<li>More manual intervention</li>\n</ul>\n<h4>In corner 2: Serverless compute (FaaS)</h4>\n<p>About a year later, AWS introduced the first serverless compute service ever: AWS Lambda.</p>\n<p>The most basic premise of a serverless setup is that the whole application‚Äîall its business logic‚Äîis implemented as <em>functions</em> and <em>events</em>.</p>\n<p>Here‚Äôs the full break-down. Applications get split up into different functionalities (or services), which are in turn triggered by events. You upload your function code and attach an event source to it.</p>\n<p>That‚Äôs basically it. The cloud provider takes care of the rest and ensures that your functions will always be available and usable, no matter what.</p>\n<p>When serverless compute was first introduced in 2014, the workloads were pretty limited and focused around smaller jobs such as image/data manipulation. But then AWS introduced the API Gateway as an event source for Lambda functions.</p>\n<p>That changed everything. It became possible to create whole APIs that were powered by serverless compute. More and more AWS services integrated with the Lambda compute offering, making it possible to build even larger, more complex, fully serverless applications.</p>\n<p>But what is a serverless application, exactly? In sum, an architecture is serverless if it has these characteristics:</p>\n<ul>\n<li>Event-driven workflow (‚ÄúIf X then Y‚Äù)</li>\n<li>Pay-per-execution</li>\n<li>Zero administration</li>\n<li>Auto-scaling</li>\n<li>Short-lived, stateless functions</li>\n</ul>\n<h5>Advantages of serverless</h5>\n<ul>\n<li>Zero administration</li>\n<li>Pay-per-execution</li>\n<li>Zero cost for idle time</li>\n<li>Auto-scaling</li>\n<li>Faster time-to-market</li>\n<li>Microservice nature ‚Äî> Clear codebase separation</li>\n<li>Significantly reduced administration and maintenance burden</li>\n</ul>\n<h5>Disadvantages of serverless</h5>\n<ul>\n<li>No standardization (though the CNCF is working on this)</li>\n<li>‚ÄúBlack box‚Äù environment</li>\n<li>Vendor lock-in</li>\n<li>Cold starts</li>\n<li>Complex apps can be hard to build</li>\n</ul>\n<h4>When to choose what?</h4>\n<p>Now it‚Äôs time for the big question:</p>\n<blockquote>\n<p>‚ÄúWhich technology should I pick for my next project‚Äù?</p>\n</blockquote>\n<p>Truthfully, it depends.</p>\n<h5>When to choose containerization</h5>\n<p>Containers are great if you need the flexibility to install and use software with specific version requirements. With containers, you can choose the underlying operating system and have full control of the installed programming language and runtime version.</p>\n<p>It‚Äôs even possible to operate containers with different software stacks throughout a large container fleet‚Äîespecially interesting if you need to migrate an old, legacy system into a containerized environment. As an added bonus, many tools for managing large-scale container set-ups (like <a href=\"https://kubernetes.io/\">Kubernetes</a>) come with all the best practices already baked in.</p>\n<p>This flexibility <em>does</em> come with an operational price tag, though. Containers still require a lot of maintenance and set-up.</p>\n<p>For maximum benefit, you‚Äôll need to split up your monolithic application into separate microservices, which in turn need to be rolled out as individual groups of containers. That means you‚Äôll need tooling that allows all those containers to talk to each other. You‚Äôll also need to do the grunt work of keeping their operating systems current with regular security fixes and other updates.</p>\n<p>While you <em>can</em> configure the container orchestration platform to automatically handle traffic fluctuations for you (a.k.a, self-healing and auto-scaling), the process of detecting those traffic pattern changes and spinning the containers up or down won‚Äôt be instantaneous. A complete shutdown where no container-related infrastructure is running at all (e.g. when there‚Äôs no traffic) will also not be possible. There will always be runtime costs.</p>\n<h5>When to choose serverless</h5>\n<p>In that vein, serverless is great if you need traffic pattern changes to be automatically detected and handled instantly. The application is even completely shut down if there‚Äôs no traffic at all. With serverless applications, you pay only for the resources you use; no usage, no costs.</p>\n<p>The serverless developer doesn‚Äôt have to care about administrating underlying infrastructure; they just need to care about the code and the business value to end users. Iteration can be more rapid, as code can be shipped faster, without set-up or provisioning. In fact, because the underlying infrastructure is abstracted, the developer may not even know what it looks like. They won‚Äôt really need to.</p>\n<p>But currently, there are some limitations with vendor support and ecosystem lock-in. Programming languages and runtimes are limited to whichever the provider supports (though there are  some workarounds (or ‚Äúshims‚Äù) available to overcome those restrictions). Event sources (which trigger all your functions) are usually services that the specific cloud provider offers.</p>\n<p>Reasoning about all the individual pieces of the application stack becomes harder when the infrastructure and the code are so separate. Serverless is a bit more new, and its tools still have room to evolve. That‚Äôs what we‚Äôre actively working on here at <a href=\"https://serverless.com/\">Serverless.com</a>, anyway. üòâ</p>\n<h4>Final verdict?</h4>\n<p>Of course, this will be an oversimplification. The real world is always more complex. But your rule of thumb?</p>\n<p>Choose containers and container orchestrators when you need flexibility, or when you need to migrate legacy services. Choose serverless when you need speed of development, automatic scaling and significantly lowered runtime costs.</p>\n<h5>Related articles:</h5>\n<ul>\n<li><a href=\"https://serverless.com/blog/why-we-switched-from-docker-to-serverless/\">Why we switched from Docker to Serverless</a></li>\n</ul>","frontmatter":{"title":"Serverless (FaaS) vs. Containers - when to pick which?","date":"October 06, 2017","description":"Docker, Kubernetes, Serverless? Let's discuss the respective ups and downs of containers and serverless."}}},"pageContext":{"slug":"/posts/2017-10-06-serverless-faas-vs-containers/","previous":{"fields":{"slug":"/posts/2017-10-04-serverless-express-rest-api/"},"frontmatter":{"title":"Deploy a REST API using Serverless, Express and Node.js"}},"next":{"fields":{"slug":"/posts/2017-10-06-definitive-guide-serverlessconf-2017-nyc/"},"frontmatter":{"title":"Your definitive guide to ServerlessConf 2017 in NYC"}}}}}